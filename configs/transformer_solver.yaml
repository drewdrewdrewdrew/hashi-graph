# Transformer with Progressive Masking (0% -> 100%)
# Full puzzle solving configuration

data:
  root_dir: "dataset/"
  size:
  difficulty: 0
  limit:

model:
  type: "transformer"
  node_embedding_dim: 64
  hidden_channels: 128
  num_layers: 6
  heads: 4
  dropout: 0.25
  use_degree: true
  use_global_meta_node: false
  use_row_col_meta: true
  use_distance: true
  use_edge_labels_as_features: true  # TOGGLE: Enable labels as features
  verifier_use_puzzle_nodes: true    # Enable pooling puzzle nodes for verification head

  # Factorized node encoder features
  use_capacity: true                     # Enable logical capacity embedding (1-8 for islands, 9/10 for meta)
  use_structural_degree: true            # Enable structural degree embedding (1-4 potential directions)
  use_unused_capacity: true              # Enable unused capacity embedding (0-8 remaining bridges)
  use_conflict_status: true              # Enable conflict status embedding (0-1 binary flag)

training:
  learning_rate: 0.001
  batch_size: 64
  epochs: 200
  device: "auto"
  num_workers: 4
  
  # TOGGLE: Progressive masking 0% -> 100%
  masking:
    enabled: true              # TOGGLE: Set to false to disable
    schedule: "cosine"         # TOGGLE: "cosine", "linear", or "constant"
    start_rate: 0.0           # TOGGLE: Starting masking rate
    end_rate: 1.0             # TOGGLE: Ending masking rate (1.0 = 100%)
    warmup_epochs: 20         # TOGGLE: Epochs before masking starts
  
  early_stopping:
    patience: 20
    min_delta: 0.0001




